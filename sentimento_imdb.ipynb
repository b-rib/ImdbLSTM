{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for classifying feelings (IMDb dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tqdm\n",
    "# import kormos\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import TextVectorization\n",
    "from aux_we import generate_training_data\n",
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "# mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\", \"/gpu:2\", \"/gpu:3\", \"/gpu:4\"])\n",
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('CPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "if not(os.path.exists('aclImdb_v1.tar.gz')):\n",
    "    print(\"===== Downloading Imdb Dataset =====\")\n",
    "    dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
    "                                    untar=True, cache_dir='.',\n",
    "                                    cache_subdir='')\n",
    "\n",
    "    dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "    train_dir = os.path.join(dataset_dir, 'train')\n",
    "    remove_dir = os.path.join(train_dir, 'unsup')\n",
    "    shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing downloaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "seed = 127\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size, validation_split=0.2,\n",
    "    subset='training', seed=seed)\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size, validation_split=0.2,\n",
    "    subset='validation', seed=seed)\n",
    "\n",
    "#Change to true to see a sample\n",
    "print_one = False\n",
    "if print_one:\n",
    "    for text_batch, label_batch in train_ds.take(1):\n",
    "        for i in range(1):\n",
    "            print(f\"Review: {text_batch.numpy()[i]}\")\n",
    "            print(f\"Label: {label_batch.numpy()[i]}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 19:05:08.273720: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  nots = tf.strings.regex_replace(lowercase, 'n\\'t', ' not')\n",
    "  ss = tf.strings.regex_replace(nots, '\\'s', '')\n",
    "  stripped_html = tf.strings.regex_replace(ss, '<br />', ' ')\n",
    "  no_ponctuation = tf.strings.regex_replace(stripped_html,'[%s]' % re.escape(string.punctuation), '')\n",
    "  single_spaces = tf.strings.regex_replace(no_ponctuation, '  ', ' ')\n",
    "  for i in range(2):\n",
    "    single_spaces = tf.strings.regex_replace(single_spaces, '  ', ' ')\n",
    "  return single_spaces\n",
    "\n",
    "dictionary_size = 500\n",
    "max_review_size = 250\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=dictionary_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_review_size)\n",
    "\n",
    "#Build dictonary\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "text_ds = text_ds.cache().prefetch(buffer_size=10)\n",
    "vectorize_layer.adapt(text_ds)\n",
    "\n",
    "#Print one\n",
    "# text_batch, label_batch = next(iter(train_ds))\n",
    "# first_review, first_label = text_batch[10], label_batch[10]\n",
    "# print(\"Review\", custom_standardization(first_review))\n",
    "# print(\"Label\", train_ds.class_names[first_label])\n",
    "# print(\"Vectorized review\", vectorize_layer(first_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review tf.Tensor(b'i never really knew who robert wuhl was before seeing this but after seeing it i realized what a funny man he is this hbo special features him teaching american history to new york university film students and the man was just phenomenal he poked fun at almost every key historic event that occurred not just in the us but some other parts of the world this documentarycomedy was a great satire that made me question if what i accept as the infallible true history is really true i enjoyed how mr wuhl managed to mix useful information with great comedy and made learning a lot more exciting i would recommend this to anyone interested in history and is willing to question what hisher beliefs', shape=(), dtype=string)\n",
      "Label tf.Tensor(1, shape=(), dtype=int32)\n",
      "Vectorized review tf.Tensor(\n",
      "[ 10 111  63   1  33   1   1  14 149 301  11  19 100 301   8  10   1  47\n",
      "   4 156 123  23   7  11   1 303   1  89   1 307 458   6 160   1   1  20\n",
      "   1   3   2 123  14  41   1  23   1 239  29 206 168   1   1   1  12   1\n",
      "  13  41   9   2 162  19  48  82   1   5   2 172  11   1  14   4  85   1\n",
      "  12  92  69   1  45  47  10   1  15   2   1 270 458   7  63 270  10 482\n",
      "  87 431   1   1   6   1   1   1  16  85 211   3  92   1   4 167  50   1\n",
      "  10  54 361  11   6 237   1   9 458   3   7   1   6   1  47   1   1   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0], shape=(250,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print_one = True\n",
    "if print_one:\n",
    "    text_batch, label_batch = next(iter(train_ds))\n",
    "    first_review, first_label = text_batch[10], label_batch[10]\n",
    "    print(\"Review\", custom_standardization(first_review))\n",
    "    print(\"Label\", first_label)\n",
    "    print(\"Vectorized review\", vectorize_layer(first_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset for training the word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:57<00:00, 350.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (1241232,)\n",
      "contexts.shape: (1241232, 9)\n",
      "labels.shape: (1241232, 9)\n"
     ]
    }
   ],
   "source": [
    "print_stuff = False\n",
    "neg_samples = 8\n",
    "\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "if print_stuff:\n",
    "    print('Part of vocab:',inverse_vocab[:200])\n",
    "\n",
    "# Vectorize the data in text_ds\n",
    "text_vector_ds = text_ds.prefetch(AUTOTUNE).map(vectorize_layer)\n",
    "lst = list(text_vector_ds.as_numpy_iterator())\n",
    "sequences = lst[0]\n",
    "for i in range(1,len(lst)):\n",
    "    arr = np.asarray(lst[i])\n",
    "    sequences = np.append(sequences, arr, 0)\n",
    "sequences.reshape(-1)\n",
    "print(sequences.shape)\n",
    "\n",
    "if print_stuff:\n",
    "    print(len(sequences)*batch_size)\n",
    "    for seq in sequences[2][:1]:\n",
    "        print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")\n",
    "\n",
    "# Create\n",
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=5,\n",
    "    neg_samples=neg_samples,\n",
    "    vocab_size=dictionary_size,\n",
    "    seed=seed)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)[:,:,0]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 9), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 9), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1212/1212 [==============================] - 5s 4ms/step - loss: 1.9601 - accuracy: 0.3504\n",
      "Epoch 2/20\n",
      "1212/1212 [==============================] - 5s 4ms/step - loss: 1.9039 - accuracy: 0.3629\n",
      "Epoch 3/20\n",
      "1212/1212 [==============================] - 4s 4ms/step - loss: 1.8906 - accuracy: 0.3680\n",
      "Epoch 4/20\n",
      "1212/1212 [==============================] - 5s 4ms/step - loss: 1.8808 - accuracy: 0.3718\n",
      "Epoch 5/20\n",
      "1212/1212 [==============================] - 5s 4ms/step - loss: 1.8735 - accuracy: 0.3742\n",
      "Epoch 6/20\n",
      "1212/1212 [==============================] - 4s 4ms/step - loss: 1.8681 - accuracy: 0.3761\n",
      "Epoch 7/20\n",
      "1212/1212 [==============================] - 4s 4ms/step - loss: 1.8642 - accuracy: 0.3774\n",
      "Epoch 8/20\n",
      "1212/1212 [==============================] - 4s 4ms/step - loss: 1.8614 - accuracy: 0.3784\n",
      "Epoch 9/20\n",
      "1212/1212 [==============================] - 4s 4ms/step - loss: 1.8592 - accuracy: 0.3791\n",
      "Epoch 10/20\n",
      "1212/1212 [==============================] - 5s 4ms/step - loss: 1.8577 - accuracy: 0.3797\n",
      "Epoch 11/20\n",
      "1212/1212 [==============================] - 4s 4ms/step - loss: 1.8564 - accuracy: 0.3799\n",
      "Epoch 12/20\n",
      "1212/1212 [==============================] - 5s 4ms/step - loss: 1.8555 - accuracy: 0.3802\n",
      "Epoch 13/20\n",
      "1212/1212 [==============================] - 5s 4ms/step - loss: 1.8547 - accuracy: 0.3804\n",
      "Epoch 14/20\n",
      "1212/1212 [==============================] - 4s 4ms/step - loss: 1.8541 - accuracy: 0.3806\n",
      "Epoch 15/20\n",
      "1212/1212 [==============================] - 5s 4ms/step - loss: 1.8536 - accuracy: 0.3808\n",
      "Epoch 16/20\n",
      "1212/1212 [==============================] - 4s 4ms/step - loss: 1.8532 - accuracy: 0.3809\n",
      "Epoch 17/20\n",
      "1212/1212 [==============================] - 5s 4ms/step - loss: 1.8528 - accuracy: 0.3810\n",
      "Epoch 18/20\n",
      "1212/1212 [==============================] - 4s 4ms/step - loss: 1.8525 - accuracy: 0.3811\n",
      "Epoch 19/20\n",
      "1212/1212 [==============================] - 4s 4ms/step - loss: 1.8523 - accuracy: 0.3812\n",
      "Epoch 20/20\n",
      "1212/1212 [==============================] - 4s 3ms/step - loss: 1.8521 - accuracy: 0.3813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x167dcf310>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim=20\n",
    "\n",
    "#Train WE\n",
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, dictionary_size=dictionary_size, embedding_dim=embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(dictionary_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(dictionary_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=neg_samples+1)\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots\n",
    "\n",
    "w2v = Word2Vec(dictionary_size, embedding_dim)\n",
    "\n",
    "w2v.compile(optimizer='adam',\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])\n",
    "            \n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "w2v.fit(dataset,\n",
    "       epochs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save weigts of trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = w2v.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save vectors and words in .tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process vectors from tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['after'] = after [-0.25236744  0.22294095 -0.47748455  0.8000439  -0.27904424  0.6006815\n",
      " -0.3504664   0.27771127 -0.18391116 -0.2848318  -0.01555239 -0.13021846\n",
      " -0.24619533 -0.2636221   0.42959565 -0.33255276 -0.0963175  -0.28762874\n",
      "  0.20503695 -0.02305496]\n",
      "200 ['right'] = right [-0.24869512  0.3069537  -0.4729333   0.45070252  0.02212025  0.27604362\n",
      " -0.0543763   0.24817306 -0.23229143  0.1652415  -0.319195   -0.26941577\n",
      " -0.34723032  0.0796494   0.15716659 -0.32968554  0.4386617  -0.44977346\n",
      "  0.19259012  0.38791212]\n",
      "300 ['effects'] = effects [-8.6872090e-01  7.1108200e-01  3.1629488e-01 -4.8028690e-01\n",
      "  1.0770323e-01  2.1325263e-01 -5.2093870e-01 -4.6367460e-01\n",
      " -1.2160134e+00  2.3402283e-02  5.6021210e-01 -8.7606170e-01\n",
      " -8.9379440e-02  7.0010360e-04  1.1745342e-01 -1.3862087e-01\n",
      "  1.2602201e-01 -5.2380973e-01 -7.4549645e-01 -4.9149744e-02]\n",
      "400 ['supposed'] = supposed [-0.22659782 -0.16609944 -0.53715944 -0.30274904 -0.168148    0.52029735\n",
      "  0.6035064   0.57658684 -0.30032948  1.6900333  -0.23354135 -0.8110227\n",
      "  0.14796105  0.22521704  0.95159537  0.15053725  0.29711515 -0.08251103\n",
      " -0.10805687  0.2132606 ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('vectors.tsv', sep=\"\\t\")\n",
    "df2 = pd.read_csv('metadata.tsv', sep=\"\\t\")\n",
    "\n",
    "vecs = df.values\n",
    "wrds = df2.values\n",
    "\n",
    "num_tokens = dictionary_size\n",
    "\n",
    "#Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for i in range(2,len(vecs)):\n",
    "    embedding_matrix[i] = vecs[i-2]\n",
    "    if (i%100==0):\n",
    "        print(i, wrds[i-2], '=', inverse_vocab[i],embedding_matrix[i])\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=True,\n",
    ")\n",
    "\n",
    "os.remove('metadata.tsv')\n",
    "os.remove('vectors.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 15s 704ms/step - loss: 0.6932 - accuracy: 0.5028 - val_loss: 0.6932 - val_accuracy: 0.5086\n",
      "Epoch 2/15\n",
      "20/20 [==============================] - 13s 666ms/step - loss: 0.6927 - accuracy: 0.5113 - val_loss: 0.6929 - val_accuracy: 0.5020\n",
      "Epoch 3/15\n",
      "20/20 [==============================] - 13s 629ms/step - loss: 0.6908 - accuracy: 0.5128 - val_loss: 0.6907 - val_accuracy: 0.5158\n",
      "Epoch 4/15\n",
      "20/20 [==============================] - 13s 647ms/step - loss: 0.6884 - accuracy: 0.5338 - val_loss: 0.6870 - val_accuracy: 0.5364\n",
      "Epoch 5/15\n",
      "20/20 [==============================] - 13s 669ms/step - loss: 0.6890 - accuracy: 0.5418 - val_loss: 0.6918 - val_accuracy: 0.5164\n",
      "Epoch 6/15\n",
      "20/20 [==============================] - 13s 656ms/step - loss: 0.6892 - accuracy: 0.5257 - val_loss: 0.6899 - val_accuracy: 0.5228\n",
      "Epoch 7/15\n",
      "20/20 [==============================] - 15s 739ms/step - loss: 0.6874 - accuracy: 0.5289 - val_loss: 0.6890 - val_accuracy: 0.5292\n",
      "Epoch 8/15\n",
      "20/20 [==============================] - 16s 754ms/step - loss: 0.6852 - accuracy: 0.5435 - val_loss: 0.6875 - val_accuracy: 0.5336\n",
      "Epoch 9/15\n",
      "20/20 [==============================] - 14s 716ms/step - loss: 0.6798 - accuracy: 0.5570 - val_loss: 0.6757 - val_accuracy: 0.5642\n",
      "Epoch 10/15\n",
      "20/20 [==============================] - 15s 755ms/step - loss: 0.6205 - accuracy: 0.6859 - val_loss: 0.6213 - val_accuracy: 0.6718\n",
      "Epoch 11/15\n",
      "20/20 [==============================] - 15s 769ms/step - loss: 0.5856 - accuracy: 0.7206 - val_loss: 0.5846 - val_accuracy: 0.7238\n",
      "Epoch 12/15\n",
      "20/20 [==============================] - 16s 798ms/step - loss: 0.5484 - accuracy: 0.7523 - val_loss: 0.5514 - val_accuracy: 0.7512\n",
      "Epoch 13/15\n",
      "20/20 [==============================] - 15s 733ms/step - loss: 0.5289 - accuracy: 0.7673 - val_loss: 0.5562 - val_accuracy: 0.7562\n",
      "Epoch 14/15\n",
      "20/20 [==============================] - 14s 707ms/step - loss: 0.5019 - accuracy: 0.7854 - val_loss: 0.5125 - val_accuracy: 0.7780\n",
      "Epoch 15/15\n",
      "20/20 [==============================] - 15s 740ms/step - loss: 0.4783 - accuracy: 0.7974 - val_loss: 0.4866 - val_accuracy: 0.7868\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 32479), started 1:22:37 ago. (Use '!kill 32479' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2feb4c6ee4940cb3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2feb4c6ee4940cb3\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1 = Sequential([\n",
    "  vectorize_layer,\n",
    "  Embedding(dictionary_size,embedding_dim),\n",
    "  LSTM(50),\n",
    "  Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# if os.path.exists(\"logs\"):\n",
    "#   os.rmdir(\"logs\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/lstm/model1\")\n",
    "\n",
    "model1.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])\n",
    "\n",
    "#docs_infra: no_execute\n",
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "20/20 [==============================] - 15s 668ms/step - loss: 0.6923 - accuracy: 0.5131 - val_loss: 0.6905 - val_accuracy: 0.5236\n",
      "Epoch 2/15\n",
      "20/20 [==============================] - 14s 681ms/step - loss: 0.6809 - accuracy: 0.5644 - val_loss: 0.6727 - val_accuracy: 0.6174\n",
      "Epoch 3/15\n",
      "20/20 [==============================] - 14s 684ms/step - loss: 0.6526 - accuracy: 0.6341 - val_loss: 0.6460 - val_accuracy: 0.6416\n",
      "Epoch 4/15\n",
      "20/20 [==============================] - 13s 674ms/step - loss: 0.6250 - accuracy: 0.6677 - val_loss: 0.6266 - val_accuracy: 0.6786\n",
      "Epoch 5/15\n",
      "20/20 [==============================] - 13s 667ms/step - loss: 0.5893 - accuracy: 0.7114 - val_loss: 0.5721 - val_accuracy: 0.7236\n",
      "Epoch 6/15\n",
      "20/20 [==============================] - 13s 670ms/step - loss: 0.5584 - accuracy: 0.7397 - val_loss: 0.5469 - val_accuracy: 0.7500\n",
      "Epoch 7/15\n",
      "20/20 [==============================] - 13s 664ms/step - loss: 0.5281 - accuracy: 0.7574 - val_loss: 0.5197 - val_accuracy: 0.7554\n",
      "Epoch 8/15\n",
      "20/20 [==============================] - 14s 695ms/step - loss: 0.4874 - accuracy: 0.7775 - val_loss: 0.4810 - val_accuracy: 0.7820\n",
      "Epoch 9/15\n",
      "20/20 [==============================] - 13s 668ms/step - loss: 0.4626 - accuracy: 0.7914 - val_loss: 0.4607 - val_accuracy: 0.7926\n",
      "Epoch 10/15\n",
      "20/20 [==============================] - 13s 662ms/step - loss: 0.4451 - accuracy: 0.8001 - val_loss: 0.4442 - val_accuracy: 0.8002\n",
      "Epoch 11/15\n",
      "20/20 [==============================] - 14s 685ms/step - loss: 0.4298 - accuracy: 0.8065 - val_loss: 0.4337 - val_accuracy: 0.8052\n",
      "Epoch 12/15\n",
      "20/20 [==============================] - 14s 728ms/step - loss: 0.4179 - accuracy: 0.8112 - val_loss: 0.4242 - val_accuracy: 0.8092\n",
      "Epoch 13/15\n",
      "20/20 [==============================] - 13s 642ms/step - loss: 0.4080 - accuracy: 0.8173 - val_loss: 0.4173 - val_accuracy: 0.8132\n",
      "Epoch 14/15\n",
      "20/20 [==============================] - 15s 739ms/step - loss: 0.4013 - accuracy: 0.8224 - val_loss: 0.4145 - val_accuracy: 0.8120\n",
      "Epoch 15/15\n",
      "20/20 [==============================] - 14s 701ms/step - loss: 0.3960 - accuracy: 0.8245 - val_loss: 0.4136 - val_accuracy: 0.8140\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 32479), started 1:26:14 ago. (Use '!kill 32479' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ee2aad57e56fbf94\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ee2aad57e56fbf94\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  embedding_layer,\n",
    "  LSTM(50),\n",
    "  Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# if os.path.exists(\"logs\"):\n",
    "#   os.rmdir(\"logs\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/lstm\")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])\n",
    "\n",
    "#docs_infra: no_execute\n",
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 215ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: b'Nightmare Weekend stars a cast of ridiculous actors with even less of an idea of what is going on than the director had, if you can imagine that. There is no decipherable plot or story, the special effects are a joke, and even the sound is terrible. This film was directed by Henry Sala. It was the only film that he ever directed, and the reason is obvious.'\n",
      "True prediction: 0.035815365612506866\tRounded prediction: 0\tTrue:0\n",
      "\n",
      "\n",
      "Review: b\"When I first saw this movie I was with my dad. He encouraged me to watch this movie because it was one of his favorites. After watching the movie it instantly became one of my favorites. <br /><br />A River Runs Through It is about two brothers who each take a different path in life. Norman Maclean (Craig Sheffer) is the older of the two brothers and he is set on the path of education. Paul Maclean (Brad Pitt) is the rebellious younger brother who travels on a path full of obstacles. The movie follows these characters as the each follow their own path.<br /><br />There is no downside to this movie. You will be entertained the whole way through. The acting, directing, and script is all perfect. The two things that are exceptional are the cinematography and the score. Both of which entrap you in the world Robert Redford creates for you. <br /><br />This is an all around great movie that is destined to be a classic. It sure is in my book. If you haven't seen this movie definitely watch it as soon as you can because it will stay with you forever.\"\n",
      "True prediction: 0.9545835852622986\tRounded prediction: 1\tTrue:1\n",
      "\n",
      "\n",
      "Review: b'The Egyptian Movies has A Lot Of Filmes With High Level Of Drama Or Romance Or Comedy Or Action Even Sports... \"Ziab la Ta\\'Kohl AL lam\" Was banned In Egypt Because It Content Nudity (Full Frontal Female Nudity) And This Kind Of Nudity Is Prohibited In The Egyptian Movies.. When I Saw this Movies I Felt Down... Fool Story.. Nude Actress.. Bad Action.. Some Horror & Awful Colors.. Dear Friend.. If You Wanna See A great Egyptian Movie...Simply: Stay Away Form \"Ziab la Ta\\'Kohl AL lam\".. We Have Great Movies In Egypt... We Have A Great Actors Who Won A Global Wins Like: Omar El Sheriff Or Gameel Rateb.. We Have Great Directors Like \"Yousef Shahin\" So Believe Me Pall.. You Don\\'t Need To See This Movie..'\n",
      "True prediction: 0.7179211974143982\tRounded prediction: 1\tTrue:0\n",
      "\n",
      "\n",
      "Review: b\"He-he-hello!! This is a really fun movie. Basically, in Party Girl, you have your fun-lovin', independent, early 90's New Yorker chick. Along with her party friends, she meets a mature Turkish Vendor. It is a comming of age story for those new adults who are searching for what they want to do. It is comforting to see a female slacker develop into a mature woman. Hope is given to all of us slackers who might feel like their only skills are being able to maintain while hammered and a nack for throwing good parties.<br /><br />On a side note, Parker Posey makes this movie great. I have never been a great fan of her, but this movie makes me just want to watch all of her movies. There are subtle manerisms that perfected her character. If you want good laughs and a fun time, make sure to watch this movie. Repeated viewings are a must.\"\n",
      "True prediction: 0.9345836639404297\tRounded prediction: 1\tTrue:1\n",
      "\n",
      "\n",
      "Review: b'Just saw a pre-screening tonight. What can I say? It lived up to it\\'s mediocre trailer run, though that\\'s saying nothing at all. It did absolutely nothing that any movie before it hasn\\'t done, and it played out in such a clich\\xc3\\xa9 fashion that eventually I got to the point where I stopped laughing only because I was laughing with the audience, and instead let the humorless movie play out.<br /><br />So let\\'s see... we have the less-than-spectacular main character that is trying to get back with his ex-girlfriend but he\\'s not good enough for her, check. We have the three buddies that all have their own \"personality\" with one being the best friend who tries to get with the main girl character\\'s best friend but is constantly rejected, another friend being the super awkward one that can\\'t live down seeing the positive in everything 24/7 and is thrown in for the one-liners (which in this case is just a bunch of movie references, specifically from Disney), and the third guy whose name you won\\'t ever remember but is there to complete the square and throw in consoling messages to whomever will care to listen... check. We have the girl\\'s ex-boyfriend and her parents ****-block the relationship at any possible means when things are looking up, not to mention the awkward family members from the main character\\'s side... check. We have the downer period an hour into the movie where everyone is depressed, check. We have the movie\\'s \"funny\" moments come from incessant swearing, people falling down or being hit, scenes from the trailer, and homosexual innuendos... check. And dare I call it a spoiler, but we have an ending that unfolds exactly as one thought that it would unfold before even seeing the movie... check.<br /><br />Honestly, this could have... no, wait... should have been a PG-13 movie. All that needed to be dropped were any F-bombs. Honestly, it would have gotten much more publicity from the crowd that enjoys this kind of humor, would have gotten less media exposure, and thusly would have not been disliked as much from people like myself who should try and hold it up higher to the recent R-rated comedies like Superbad and Knocked Up. The humor in this movie is just so awkward that it doesn\\'t fit in with what general people look for. I bet even the actors were often times unsettled with some of the dialogue and action they had to deliver on camera. Let\\'s put it this way... in the theater, it will help you laugh because it\\'s on the big screen and others are laughing. When this movie hits Showtime and you\\'re checking it out at 2:00 PM on an off-day, you may be inclined to change the channel. The only thing that will keep you watching is Alice Eve\\'s hotness (who is not quite a 10, but still very good looking).<br /><br />Aside from the main resolution, this film kicked a lot of subplots to the side of the curb and seemed to forget to write more story that they tried to develop in the beginning of the movie, where everything else pretty much flies out the window. So there is a main resolution, but what comes of it? It\\'s never really clear-cut, nor does it allow the ending to be \"feel-good\" with the abruptness.<br /><br />There was only one thing worth nothing in this movie, and that was the good soundtrack. Aside from the nice choice of 90\\'s alternative rock songs, there was a nice upbeat score that would play in some parts of the movie (more so the beginning of it) that reminds me of something David Holmes would mix up/compose. I\\'ll give them props for a great choice of sound.<br /><br />One last thing, this movie was probably filmed sometime late last summer, because the inadvertent yet proud Pepsi sponsorship showed the yellow bottle caps that they had during that Rock Band promotion. I just figured a lot of Rock Band gamers would catch onto that one if you saw it. But I say hold onto your money. If this was PG-13 and you were 15 years old on a Friday night with a group of friends, I\\'d say knock yourselves out. Otherwise, definitely pass. It doesn\\'t try and compete with the R-rated movies of the past few years, and ideally it definitely isn\\'t as good.'\n",
      "True prediction: 0.19758467376232147\tRounded prediction: 0\tTrue:0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vali = val_ds.as_numpy_iterator()\n",
    "text_batch, label_batch = next(iter(val_ds))\n",
    "\n",
    "for i in range(0,500,100):\n",
    "    print(f\"Review: {text_batch[i]}\\nTrue prediction: {predictions[i][0]}\\tRounded prediction: {round(predictions[i][0])}\\tTrue:{label_batch[i]}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](loss%20per%20epoch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A few predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review: b'Nightmare Weekend stars a cast of ridiculous actors with even less of an idea of what is going on than the director had, if you can imagine that. There is no decipherable plot or story, the special effects are a joke, and even the sound is terrible. This film was directed by Henry Sala. It was the only film that he ever directed, and the reason is obvious.'\n",
    "\n",
    "True prediction: 0.035815365612506866\tRounded prediction: 0\tTrue:0\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "Review: b\"When I first saw this movie I was with my dad. He encouraged me to watch this movie because it was one of his favorites. After watching the movie it instantly became one of my favorites. A River Runs Through It is about two brothers who each take a different path in life. Norman Maclean (Craig Sheffer) is the older of the two brothers and he is set on the path of education. Paul Maclean (Brad Pitt) is the rebellious younger brother who travels on a path full of obstacles. The movie follows these characters as the each follow their own path. There is no downside to this movie. You will be entertained the whole way through. The acting, directing, and script is all perfect. The two things that are exceptional are the cinematography and the score. Both of which entrap you in the world Robert Redford creates for you. This is an all around great movie that is destined to be a classic. It sure is in my book. If you haven't seen this movie definitely watch it as soon as you can because it will stay with you forever.\"\n",
    "\n",
    "True prediction: 0.9545835852622986\tRounded prediction: 1\tTrue:1\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "Review: b'The Egyptian Movies has A Lot Of Filmes With High Level Of Drama Or Romance Or Comedy Or Action Even Sports... \"Ziab la Ta\\'Kohl AL lam\" Was banned In Egypt Because It Content Nudity (Full Frontal Female Nudity) And This Kind Of Nudity Is Prohibited In The Egyptian Movies.. When I Saw this Movies I Felt Down... Fool Story.. Nude Actress.. Bad Action.. Some Horror & Awful Colors.. Dear Friend.. If You Wanna See A great Egyptian Movie...Simply: Stay Away Form \"Ziab la Ta\\'Kohl AL lam\".. We Have Great Movies In Egypt... We Have A Great Actors Who Won A Global Wins Like: Omar El Sheriff Or Gameel Rateb.. We Have Great Directors Like \"Yousef Shahin\" So Believe Me Pall.. You Don\\'t Need To See This Movie..'\n",
    "\n",
    "True prediction: 0.7179211974143982\tRounded prediction: 1\tTrue:0\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "Review: b\"He-he-hello!! This is a really fun movie. Basically, in Party Girl, you have your fun-lovin', independent, early 90's New Yorker chick. Along with her party friends, she meets a mature Turkish Vendor. It is a comming of age story for those new adults who are searching for what they want to do. It is comforting to see a female slacker develop into a mature woman. Hope is given to all of us slackers who might feel like their only skills are being able to maintain while hammered and a nack for throwing good parties. On a side note, Parker Posey makes this movie great. I have never been a great fan of her, but this movie makes me just want to watch all of her movies. There are subtle manerisms that perfected her character. If you want good laughs and a fun time, make sure to watch this movie. Repeated viewings are a must.\"\n",
    "\n",
    "True prediction: 0.9345836639404297\tRounded prediction: 1\tTrue:1\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "Review: b'Just saw a pre-screening tonight. What can I say? It lived up to it\\'s mediocre trailer run, though that\\'s saying nothing at all. It did absolutely nothing that any movie before it hasn\\'t done, and it played out in such a clich\\xc3\\xa9 fashion that eventually I got to the point where I stopped laughing only because I was laughing with the audience, and instead let the humorless movie play out. So let\\'s see... we have the less-than-spectacular main character that is trying to get back with his ex-girlfriend but he\\'s not good enough for her, check. We have the three buddies that all have their own \"personality\" with one being the best friend who tries to get with the main girl character\\'s best friend but is constantly rejected, another friend being the super awkward one that can\\'t live down seeing the positive in everything 24/7 and is thrown in for the one-liners (which in this case is just a bunch of movie references, specifically from Disney), and the third guy whose name you won\\'t ever remember but is there to complete the square and throw in consoling messages to whomever will care to listen... check. We have the girl\\'s ex-boyfriend and her parents ****-block the relationship at any possible means when things are looking up, not to mention the awkward family members from the main character\\'s side... check. We have the downer period an hour into the movie where everyone is depressed, check. We have the movie\\'s \"funny\" moments come from incessant swearing, people falling down or being hit, scenes from the trailer, and homosexual innuendos... check. And dare I call it a spoiler, but we have an ending that unfolds exactly as one thought that it would unfold before even seeing the movie... check. Honestly, this could have... no, wait... should have been a PG-13 movie. All that needed to be dropped were any F-bombs. Honestly, it would have gotten much more publicity from the crowd that enjoys this kind of humor, would have gotten less media exposure, and thusly would have not been disliked as much from people like myself who should try and hold it up higher to the recent R-rated comedies like Superbad and Knocked Up. The humor in this movie is just so awkward that it doesn\\'t fit in with what general people look for. I bet even the actors were often times unsettled with some of the dialogue and action they had to deliver on camera. Let\\'s put it this way... in the theater, it will help you laugh because it\\'s on the big screen and others are laughing. When this movie hits Showtime and you\\'re checking it out at 2:00 PM on an off-day, you may be inclined to change the channel. The only thing that will keep you watching is Alice Eve\\'s hotness (who is not quite a 10, but still very good looking). Aside from the main resolution, this film kicked a lot of subplots to the side of the curb and seemed to forget to write more story that they tried to develop in the beginning of the movie, where everything else pretty much flies out the window. So there is a main resolution, but what comes of it? It\\'s never really clear-cut, nor does it allow the ending to be \"feel-good\" with the abruptness. There was only one thing worth nothing in this movie, and that was the good soundtrack. Aside from the nice choice of 90\\'s alternative rock songs, there was a nice upbeat score that would play in some parts of the movie (more so the beginning of it) that reminds me of something David Holmes would mix up/compose. I\\'ll give them props for a great choice of sound. One last thing, this movie was probably filmed sometime late last summer, because the inadvertent yet proud Pepsi sponsorship showed the yellow bottle caps that they had during that Rock Band promotion. I just figured a lot of Rock Band gamers would catch onto that one if you saw it. But I say hold onto your money. If this was PG-13 and you were 15 years old on a Friday night with a group of friends, I\\'d say knock yourselves out. Otherwise, definitely pass. It doesn\\'t try and compete with the R-rated movies of the past few years, and ideally it definitely isn\\'t as good.'\n",
    "\n",
    "True prediction: 0.19758467376232147\tRounded prediction: 0\tTrue:0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
