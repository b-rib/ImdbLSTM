{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for classifying feelings (IMDb dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tqdm\n",
    "# import kormos\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from keras.layers import TextVectorization\n",
    "from aux_we import generate_training_data\n",
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Downloading Imdb Dataset =====\n",
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n"
     ]
    }
   ],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "if not(os.path.exists('aclImdb_v1.tar.gz')):\n",
    "    print(\"===== Downloading Imdb Dataset =====\")\n",
    "    dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
    "                                    untar=True, cache_dir='.',\n",
    "                                    cache_subdir='')\n",
    "\n",
    "    dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "    train_dir = os.path.join(dataset_dir, 'train')\n",
    "    remove_dir = os.path.join(train_dir, 'unsup')\n",
    "    shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing downloaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "seed = 127\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size, validation_split=0.2,\n",
    "    subset='training', seed=seed)\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size, validation_split=0.2,\n",
    "    subset='validation', seed=seed)\n",
    "\n",
    "#Change to true to see a sample\n",
    "print_one = False\n",
    "if print_one:\n",
    "    for text_batch, label_batch in train_ds.take(1):\n",
    "        for i in range(1):\n",
    "            print(f\"Review: {text_batch.numpy()[i]}\")\n",
    "            print(f\"Label: {label_batch.numpy()[i]}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  nots = tf.strings.regex_replace(lowercase, 'n\\'t', ' not')\n",
    "  ss = tf.strings.regex_replace(nots, '\\'s', ' s')\n",
    "  stripped_html = tf.strings.regex_replace(ss, '<br />', ' ')\n",
    "  no_ponctuation = tf.strings.regex_replace(stripped_html,'[%s]' % re.escape(string.punctuation), '')\n",
    "  single_spaces = tf.strings.regex_replace(no_ponctuation, '  ', ' ')\n",
    "  for i in range(2):\n",
    "    single_spaces = tf.strings.regex_replace(single_spaces, '  ', ' ')\n",
    "  return single_spaces\n",
    "\n",
    "dictionary_size = 10000\n",
    "max_review_size = 250\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=dictionary_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_review_size)\n",
    "\n",
    "#Build dictonary\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "text_ds = text_ds.cache().prefetch(buffer_size=10)\n",
    "vectorize_layer.adapt(text_ds)\n",
    "\n",
    "#Print one\n",
    "# text_batch, label_batch = next(iter(train_ds))\n",
    "# first_review, first_label = text_batch[10], label_batch[10]\n",
    "# print(\"Review\", custom_standardization(first_review))\n",
    "# print(\"Label\", train_ds.class_names[first_label])\n",
    "# print(\"Vectorized review\", vectorize_layer(first_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review tf.Tensor(b'i never really knew who robert wuhl was before seeing this but after seeing it i realized what a funny man he is this hbo special features him teaching american history to new york university film students and the man was just phenomenal he poked fun at almost every key historic event that occurred not just in the us but some other parts of the world this documentarycomedy was a great satire that made me question if what i accept as the infallible true history is really true i enjoyed how mr wuhl managed to mix useful information with great comedy and made learning a lot more exciting i would recommend this to anyone interested in history and is willing to question what hisher beliefs', shape=(), dtype=string)\n",
      "Label tf.Tensor(1, shape=(), dtype=int32)\n",
      "Vectorized review tf.Tensor(\n",
      "[  10  112   64  660   34  649    1   15  150  302   11   20  101  302\n",
      "    8   10 1693   48    4  157  124   24    7   11 4035  303  926   90\n",
      " 5320  308  459    6  161  739 3375   21 1510    3    2  124   15   42\n",
      " 6498   24    1  240   30  207  169 1450 5492 1472   12 3870   13   42\n",
      "    9    2  163   20   49   83  504    5    2  173   11    1   15    4\n",
      "   86 1916   12   93   70  859   46   48   10 1685   16    2    1  271\n",
      "  459    7   64  271   10  483   88  433    1 1219    6 1449 4564 1556\n",
      "   17   86  212    3   93 2743    4  168   51 1074   10   55  362   11\n",
      "    6  238  898    9  459    3    7 1676    6  859   48 8850 4319    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0], shape=(250,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print_one = False\n",
    "if print_one:\n",
    "    text_batch, label_batch = next(iter(train_ds))\n",
    "    first_review, first_label = text_batch[10], label_batch[10]\n",
    "    print(\"Review\", custom_standardization(first_review))\n",
    "    print(\"Label\", first_label)\n",
    "    print(\"Vectorized review\", vectorize_layer(first_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset for training the word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [02:24<00:00, 137.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (1983779,)\n",
      "contexts.shape: (1983779, 5)\n",
      "labels.shape: (1983779, 5)\n"
     ]
    }
   ],
   "source": [
    "print_stuff = False\n",
    "\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "if print_stuff:\n",
    "    print('Part of vocab:',inverse_vocab[:200])\n",
    "\n",
    "# Vectorize the data in text_ds\n",
    "text_vector_ds = text_ds.prefetch(AUTOTUNE).map(vectorize_layer)\n",
    "lst = list(text_vector_ds.as_numpy_iterator())\n",
    "sequences = lst[0]\n",
    "for i in range(1,len(lst)):\n",
    "    arr = np.asarray(lst[i])\n",
    "    sequences = np.append(sequences, arr, 0)\n",
    "sequences.reshape(-1)\n",
    "print(sequences.shape)\n",
    "\n",
    "if print_stuff:\n",
    "    print(len(sequences)*batch_size)\n",
    "    for seq in sequences[2][:1]:\n",
    "        print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")\n",
    "\n",
    "# Create\n",
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    neg_samples=4,\n",
    "    vocab_size=dictionary_size,\n",
    "    seed=seed)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)[:,:,0]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/brenda/ImdbLSTM/sentimento_imdb.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B150.162.14.45/home/brenda/ImdbLSTM/sentimento_imdb.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m neg_samples\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B150.162.14.45/home/brenda/ImdbLSTM/sentimento_imdb.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#Train WE\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B150.162.14.45/home/brenda/ImdbLSTM/sentimento_imdb.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mWord2Vec\u001b[39;00m(tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mModel):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B150.162.14.45/home/brenda/ImdbLSTM/sentimento_imdb.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m   \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, dictionary_size\u001b[39m=\u001b[39mdictionary_size, embedding_dim\u001b[39m=\u001b[39membedding_dim):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B150.162.14.45/home/brenda/ImdbLSTM/sentimento_imdb.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39msuper\u001b[39m(Word2Vec, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_dim=4\n",
    "neg_samples=2\n",
    "\n",
    "#Train WE\n",
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, dictionary_size=dictionary_size, embedding_dim=embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(dictionary_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(dictionary_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=neg_samples+1)\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots\n",
    "\n",
    "w2v = Word2Vec(dictionary_size, embedding_dim)\n",
    "\n",
    "w2v.compile(optimizer='adam',\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])\n",
    "            \n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "w2v.fit(dataset,\n",
    "       epochs=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save weigts of trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = w2v.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save vectors and words in .tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process vectors from tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('vectors.tsv', sep=\"\\t\")\n",
    "\n",
    "vecs = df.values\n",
    "\n",
    "num_tokens = dictionary_size\n",
    "\n",
    "#Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word in range(1,len(vecs)+1):\n",
    "    embedding_matrix[word] = vecs[word-1]\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")\n",
    "\n",
    "os.remove('metadata.tsv')\n",
    "os.remove('vectors.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "20/20 [==============================] - 3s 126ms/step - loss: 0.6935 - accuracy: 0.4995 - val_loss: 0.6930 - val_accuracy: 0.5020\n",
      "Epoch 2/15\n",
      "20/20 [==============================] - 3s 142ms/step - loss: 0.6930 - accuracy: 0.4995 - val_loss: 0.6928 - val_accuracy: 0.5020\n",
      "Epoch 3/15\n",
      "20/20 [==============================] - 2s 116ms/step - loss: 0.6928 - accuracy: 0.4995 - val_loss: 0.6926 - val_accuracy: 0.5020\n",
      "Epoch 4/15\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 0.6926 - accuracy: 0.4995 - val_loss: 0.6925 - val_accuracy: 0.5020\n",
      "Epoch 5/15\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 0.6924 - accuracy: 0.4995 - val_loss: 0.6924 - val_accuracy: 0.5020\n",
      "Epoch 6/15\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 0.6923 - accuracy: 0.4995 - val_loss: 0.6923 - val_accuracy: 0.5020\n",
      "Epoch 7/15\n",
      "20/20 [==============================] - 2s 106ms/step - loss: 0.6922 - accuracy: 0.4995 - val_loss: 0.6922 - val_accuracy: 0.5020\n",
      "Epoch 8/15\n",
      "20/20 [==============================] - 2s 115ms/step - loss: 0.6921 - accuracy: 0.4995 - val_loss: 0.6921 - val_accuracy: 0.5020\n",
      "Epoch 9/15\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 0.6920 - accuracy: 0.4995 - val_loss: 0.6921 - val_accuracy: 0.5020\n",
      "Epoch 10/15\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.6919 - accuracy: 0.4995 - val_loss: 0.6920 - val_accuracy: 0.5020\n",
      "Epoch 11/15\n",
      "20/20 [==============================] - 2s 94ms/step - loss: 0.6918 - accuracy: 0.4995 - val_loss: 0.6919 - val_accuracy: 0.5020\n",
      "Epoch 12/15\n",
      "20/20 [==============================] - 2s 98ms/step - loss: 0.6917 - accuracy: 0.4995 - val_loss: 0.6918 - val_accuracy: 0.5020\n",
      "Epoch 13/15\n",
      "20/20 [==============================] - 2s 98ms/step - loss: 0.6916 - accuracy: 0.4995 - val_loss: 0.6917 - val_accuracy: 0.5020\n",
      "Epoch 14/15\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.6915 - accuracy: 0.4995 - val_loss: 0.6916 - val_accuracy: 0.5020\n",
      "Epoch 15/15\n",
      "20/20 [==============================] - 2s 93ms/step - loss: 0.6913 - accuracy: 0.4995 - val_loss: 0.6916 - val_accuracy: 0.5020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 69154), started 2 days, 14:04:49 ago. (Use '!kill 69154' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-dbd7d0b4db9379fe\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-dbd7d0b4db9379fe\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  embedding_layer,\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(16, activation='relu'),\n",
    "  Dense(1)\n",
    "])\n",
    "\n",
    "os.rmdir(\"logs\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])\n",
    "\n",
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3caeb8b0f137d67a92c8e1167310073f0bfb2e774977601549773a9d0ef48812"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
