{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for classifying feelings (IMDb dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tqdm\n",
    "# import kormos\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, LSTM, Flatten\n",
    "from keras.layers import TextVectorization\n",
    "from aux_we import generate_training_data\n",
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "# mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\", \"/gpu:2\", \"/gpu:3\", \"/gpu:4\"])\n",
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('CPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "if not(os.path.exists('aclImdb_v1.tar.gz')):\n",
    "    print(\"===== Downloading Imdb Dataset =====\")\n",
    "    dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
    "                                    untar=True, cache_dir='.',\n",
    "                                    cache_subdir='')\n",
    "\n",
    "    dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "    train_dir = os.path.join(dataset_dir, 'train')\n",
    "    remove_dir = os.path.join(train_dir, 'unsup')\n",
    "    shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing downloaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "seed = 127\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size, validation_split=0.2,\n",
    "    subset='training', seed=seed)\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size, validation_split=0.2,\n",
    "    subset='validation', seed=seed)\n",
    "\n",
    "#Change to true to see a sample\n",
    "print_one = False\n",
    "if print_one:\n",
    "    for text_batch, label_batch in train_ds.take(1):\n",
    "        for i in range(1):\n",
    "            print(f\"Review: {text_batch.numpy()[i]}\")\n",
    "            print(f\"Label: {label_batch.numpy()[i]}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 18:16:38.355600: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  nots = tf.strings.regex_replace(lowercase, 'n\\'t', ' not')\n",
    "  ss = tf.strings.regex_replace(nots, '\\'s', '')\n",
    "  stripped_html = tf.strings.regex_replace(ss, '<br />', ' ')\n",
    "  no_ponctuation = tf.strings.regex_replace(stripped_html,'[%s]' % re.escape(string.punctuation), '')\n",
    "  single_spaces = tf.strings.regex_replace(no_ponctuation, '  ', ' ')\n",
    "  for i in range(2):\n",
    "    single_spaces = tf.strings.regex_replace(single_spaces, '  ', ' ')\n",
    "  return single_spaces\n",
    "\n",
    "dictionary_size = 500\n",
    "max_review_size = 250\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=dictionary_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_review_size)\n",
    "\n",
    "#Build dictonary\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "text_ds = text_ds.cache().prefetch(buffer_size=10)\n",
    "vectorize_layer.adapt(text_ds)\n",
    "\n",
    "#Print one\n",
    "# text_batch, label_batch = next(iter(train_ds))\n",
    "# first_review, first_label = text_batch[10], label_batch[10]\n",
    "# print(\"Review\", custom_standardization(first_review))\n",
    "# print(\"Label\", train_ds.class_names[first_label])\n",
    "# print(\"Vectorized review\", vectorize_layer(first_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review tf.Tensor(b'i never really knew who robert wuhl was before seeing this but after seeing it i realized what a funny man he is this hbo special features him teaching american history to new york university film students and the man was just phenomenal he poked fun at almost every key historic event that occurred not just in the us but some other parts of the world this documentarycomedy was a great satire that made me question if what i accept as the infallible true history is really true i enjoyed how mr wuhl managed to mix useful information with great comedy and made learning a lot more exciting i would recommend this to anyone interested in history and is willing to question what hisher beliefs', shape=(), dtype=string)\n",
      "Label tf.Tensor(1, shape=(), dtype=int32)\n",
      "Vectorized review tf.Tensor(\n",
      "[ 10 111  63   1  33   1   1  14 149 301  11  19 100 301   8  10   1  47\n",
      "   4 156 123  23   7  11   1 303   1  89   1 307 458   6 160   1   1  20\n",
      "   1   3   2 123  14  41   1  23   1 239  29 206 168   1   1   1  12   1\n",
      "  13  41   9   2 162  19  48  82   1   5   2 172  11   1  14   4  85   1\n",
      "  12  92  69   1  45  47  10   1  15   2   1 270 458   7  63 270  10 482\n",
      "  87 431   1   1   6   1   1   1  16  85 211   3  92   1   4 167  50   1\n",
      "  10  54 361  11   6 237   1   9 458   3   7   1   6   1  47   1   1   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0], shape=(250,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print_one = True\n",
    "if print_one:\n",
    "    text_batch, label_batch = next(iter(train_ds))\n",
    "    first_review, first_label = text_batch[10], label_batch[10]\n",
    "    print(\"Review\", custom_standardization(first_review))\n",
    "    print(\"Label\", first_label)\n",
    "    print(\"Vectorized review\", vectorize_layer(first_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset for training the word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [01:20<00:00, 247.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (1238736,)\n",
      "contexts.shape: (1238736, 101)\n",
      "labels.shape: (1238736, 101)\n"
     ]
    }
   ],
   "source": [
    "print_stuff = False\n",
    "neg_samples = 100\n",
    "\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "if print_stuff:\n",
    "    print('Part of vocab:',inverse_vocab[:200])\n",
    "\n",
    "# Vectorize the data in text_ds\n",
    "text_vector_ds = text_ds.prefetch(AUTOTUNE).map(vectorize_layer)\n",
    "lst = list(text_vector_ds.as_numpy_iterator())\n",
    "sequences = lst[0]\n",
    "for i in range(1,len(lst)):\n",
    "    arr = np.asarray(lst[i])\n",
    "    sequences = np.append(sequences, arr, 0)\n",
    "sequences.reshape(-1)\n",
    "print(sequences.shape)\n",
    "\n",
    "if print_stuff:\n",
    "    print(len(sequences)*batch_size)\n",
    "    for seq in sequences[2][:1]:\n",
    "        print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")\n",
    "\n",
    "# Create\n",
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=5,\n",
    "    neg_samples=neg_samples,\n",
    "    vocab_size=dictionary_size,\n",
    "    seed=seed)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)[:,:,0]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 101), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 101), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1209/1209 [==============================] - 7s 5ms/step - loss: 4.0141 - accuracy: 0.2734\n",
      "Epoch 2/2\n",
      "1209/1209 [==============================] - 7s 5ms/step - loss: 3.7413 - accuracy: 0.2838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15cdd61d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim=4\n",
    "\n",
    "#Train WE\n",
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, dictionary_size=dictionary_size, embedding_dim=embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(dictionary_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(dictionary_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=neg_samples+1)\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots\n",
    "\n",
    "w2v = Word2Vec(dictionary_size, embedding_dim)\n",
    "\n",
    "w2v.compile(optimizer='adam',\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])\n",
    "            \n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "w2v.fit(dataset,\n",
    "       epochs=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save weigts of trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = w2v.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save vectors and words in .tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process vectors from tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ['i'] = i [ 0.54804885 -0.49318093 -0.58473796  0.5114748 ]\n",
      "20 ['film'] = film [ 0.61194366 -0.5633387  -0.6212554   0.62394625]\n",
      "30 ['all'] = all [ 0.55780363 -0.5729802  -0.59353685  0.6133544 ]\n",
      "40 ['or'] = or [ 0.6076582 -0.5825299 -0.5710637  0.6185312]\n",
      "50 ['more'] = more [ 0.56771666 -0.6696513  -0.5372741   0.6229178 ]\n",
      "60 ['which'] = which [ 0.61152905 -0.6384574  -0.64311516  0.6083362 ]\n",
      "70 ['did'] = did [ 0.59142256 -0.524214   -0.5511373   0.5434798 ]\n",
      "80 ['get'] = get [ 0.55965173 -0.54531926 -0.60965157  0.55570817]\n",
      "90 ['most'] = most [ 0.630753   -0.6164144  -0.5603311   0.62022763]\n",
      "100 ['after'] = after [ 0.624686   -0.63663393 -0.5651988   0.6253247 ]\n",
      "110 ['plot'] = plot [ 0.59741545 -0.59914213 -0.6299575   0.65324765]\n",
      "120 ['ever'] = ever [ 0.59768796 -0.56610227 -0.51649904  0.52790487]\n",
      "130 ['should'] = should [ 0.5859335 -0.5634419 -0.5330011  0.5589434]\n",
      "140 ['back'] = back [ 0.5582274  -0.5864925  -0.59653825  0.6087693 ]\n",
      "150 ['another'] = another [ 0.60563797 -0.5720872  -0.6373603   0.58295393]\n",
      "160 ['new'] = new [ 0.6586154  -0.6055736  -0.57414746  0.6474853 ]\n",
      "170 ['quite'] = quite [ 0.57730263 -0.55662346 -0.57962465  0.60494035]\n",
      "180 ['down'] = down [ 0.55800253 -0.6316381  -0.6247684   0.61549824]\n",
      "190 ['give'] = give [ 0.6235968 -0.5383292 -0.5159581  0.6098191]\n",
      "200 ['right'] = right [ 0.5767445  -0.60623544 -0.54409325  0.57404745]\n",
      "210 ['point'] = point [ 0.53478086 -0.6137418  -0.56959003  0.6067166 ]\n",
      "220 ['guy'] = guy [ 0.57011944 -0.57284003 -0.5893902   0.6556227 ]\n",
      "230 ['am'] = am [ 0.6424563  -0.51388276 -0.5393914   0.6075224 ]\n",
      "240 ['sure'] = sure [ 0.54002416 -0.5551232  -0.51251984  0.5832215 ]\n",
      "250 ['although'] = although [ 0.65005827 -0.5725697  -0.6091886   0.5878852 ]\n",
      "260 ['place'] = place [ 0.63238823 -0.6205002  -0.5781187   0.5794266 ]\n",
      "270 ['true'] = true [ 0.5532042 -0.6253347 -0.5490497  0.611632 ]\n",
      "280 ['main'] = main [ 0.5871376 -0.5984011 -0.5794756  0.5334256]\n",
      "290 ['plays'] = plays [ 0.71386796 -0.62333673 -0.6739752   0.7003498 ]\n",
      "300 ['effects'] = effects [ 0.5477665  -0.629116   -0.57458746  0.6154153 ]\n",
      "310 ['father'] = father [ 0.64982796 -0.60863304 -0.6204418   0.6548706 ]\n",
      "320 ['men'] = men [ 0.5481997  -0.60546744 -0.642795    0.6310361 ]\n",
      "330 ['hollywood'] = hollywood [ 0.5920464  -0.6370065  -0.56339705  0.59373784]\n",
      "340 ['until'] = until [ 0.62529814 -0.56156707 -0.58921623  0.648655  ]\n",
      "350 ['truly'] = truly [ 0.6327265  -0.6221012  -0.56082296  0.6453237 ]\n",
      "360 ['next'] = next [ 0.61403394 -0.66983694 -0.61555177  0.5964115 ]\n",
      "370 ['stars'] = stars [ 0.66189593 -0.62529975 -0.6046205   0.6349943 ]\n",
      "380 ['video'] = video [ 0.5678665  -0.6228419  -0.56433207  0.6063967 ]\n",
      "390 ['piece'] = piece [ 0.6633718  -0.6429437  -0.5690554   0.68138707]\n",
      "400 ['supposed'] = supposed [ 0.57451206 -0.5589385  -0.56202525  0.6212927 ]\n",
      "410 ['sort'] = sort [ 0.58664036 -0.61170787 -0.6286258   0.60732895]\n",
      "420 ['title'] = title [ 0.64142185 -0.6017179  -0.6454342   0.6750242 ]\n",
      "430 ['seemed'] = seemed [ 0.590577   -0.5885147  -0.6105314   0.52918863]\n",
      "440 ['becomes'] = becomes [ 0.62873393 -0.66927993 -0.62714565  0.72626686]\n",
      "450 ['despite'] = despite [ 0.6157821  -0.60383487 -0.626898    0.5984268 ]\n",
      "460 ['turn'] = turn [ 0.6037969 -0.5244521 -0.6137526  0.559703 ]\n",
      "470 ['guys'] = guys [ 0.60634476 -0.60510236 -0.6026244   0.61656755]\n",
      "480 ['favorite'] = favorite [ 0.5840986  -0.5248759  -0.5067226   0.61849004]\n",
      "490 ['turns'] = turns [ 0.603371  -0.5823622 -0.594539   0.6173203]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('vectors.tsv', sep=\"\\t\")\n",
    "df2 = pd.read_csv('metadata.tsv', sep=\"\\t\")\n",
    "\n",
    "vecs = df.values\n",
    "wrds = df2.values\n",
    "\n",
    "num_tokens = dictionary_size\n",
    "\n",
    "#Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for i in range(2,len(vecs)):\n",
    "    embedding_matrix[i] = vecs[i-2]\n",
    "    if (i%10==0):\n",
    "        print(i, wrds[i-2], '=', inverse_vocab[i],embedding_matrix[i])\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")\n",
    "\n",
    "# os.remove('metadata.tsv')\n",
    "# os.remove('vectors.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "20/20 [==============================] - 4s 125ms/step - loss: 0.6939 - accuracy: 0.5087 - val_loss: 0.6942 - val_accuracy: 0.4980\n",
      "Epoch 2/15\n",
      "20/20 [==============================] - 2s 103ms/step - loss: 0.6933 - accuracy: 0.5072 - val_loss: 0.6937 - val_accuracy: 0.4984\n",
      "Epoch 3/15\n",
      "20/20 [==============================] - 2s 103ms/step - loss: 0.6932 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4994\n",
      "Epoch 4/15\n",
      "20/20 [==============================] - 2s 106ms/step - loss: 0.6931 - accuracy: 0.5088 - val_loss: 0.6933 - val_accuracy: 0.4992\n",
      "Epoch 5/15\n",
      "20/20 [==============================] - 2s 98ms/step - loss: 0.6931 - accuracy: 0.5087 - val_loss: 0.6932 - val_accuracy: 0.4998\n",
      "Epoch 6/15\n",
      "20/20 [==============================] - 2s 98ms/step - loss: 0.6931 - accuracy: 0.5082 - val_loss: 0.6932 - val_accuracy: 0.4998\n",
      "Epoch 7/15\n",
      "20/20 [==============================] - 2s 99ms/step - loss: 0.6931 - accuracy: 0.5088 - val_loss: 0.6932 - val_accuracy: 0.4998\n",
      "Epoch 8/15\n",
      "20/20 [==============================] - 2s 98ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6932 - val_accuracy: 0.4996\n",
      "Epoch 9/15\n",
      "20/20 [==============================] - 2s 100ms/step - loss: 0.6930 - accuracy: 0.5098 - val_loss: 0.6932 - val_accuracy: 0.5008\n",
      "Epoch 10/15\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.6930 - accuracy: 0.5099 - val_loss: 0.6932 - val_accuracy: 0.4986\n",
      "Epoch 11/15\n",
      "20/20 [==============================] - 2s 103ms/step - loss: 0.6930 - accuracy: 0.5092 - val_loss: 0.6932 - val_accuracy: 0.5008\n",
      "Epoch 12/15\n",
      "20/20 [==============================] - 2s 113ms/step - loss: 0.6930 - accuracy: 0.5091 - val_loss: 0.6931 - val_accuracy: 0.5054\n",
      "Epoch 13/15\n",
      "20/20 [==============================] - 2s 111ms/step - loss: 0.6930 - accuracy: 0.5083 - val_loss: 0.6931 - val_accuracy: 0.5060\n",
      "Epoch 14/15\n",
      "20/20 [==============================] - 2s 103ms/step - loss: 0.6929 - accuracy: 0.5087 - val_loss: 0.6930 - val_accuracy: 0.5072\n",
      "Epoch 15/15\n",
      "20/20 [==============================] - 2s 102ms/step - loss: 0.6929 - accuracy: 0.5103 - val_loss: 0.6929 - val_accuracy: 0.5086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 32479), started 0:21:15 ago. (Use '!kill 32479' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-245e760f2adcb8b4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-245e760f2adcb8b4\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  embedding_layer,\n",
    "  LSTM(4),\n",
    "  Dense(15),\n",
    "  Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# if os.path.exists(\"logs\"):\n",
    "#   os.rmdir(\"logs\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])\n",
    "\n",
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
